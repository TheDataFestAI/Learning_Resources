{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheDataFestAI/learning_ai_ml/blob/main/learning_notebooks/learn_essential_ml_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learn AI/ML Models"
      ],
      "metadata": {
        "id": "gHVMvlC26Tzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Vs Unsupervised Model:"
      ],
      "metadata": {
        "id": "TwxTnL-vIJR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Id | Supervised Model | Unsupervised Model\n",
        ":-- | :-------------- | :-----------------\n",
        "1 | In this model, we **have labeled data** | Here we **don't have labeled data**\n",
        "2 | Supervised Models are further classified as **Regression** and **Classification** | Unsupervised models has no further types."
      ],
      "metadata": {
        "id": "9gr4Xpn9IQts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Vs Classfication Model:"
      ],
      "metadata": {
        "id": "WzAhauEyJ5Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Id | Regression Model | Classification Model\n",
        ":-- | :-------------- | :-----------------\n",
        "1 | In this model, dependent/output variable **(y) will be continuas** | dependent/output variable **(y) will be discreate**\n",
        "2 | This model doesn't have further types | Classification can be **binary** or **multi-class classification**, based on (y) values\n",
        "3 | Example: Linear Regression, Random Forest, XG Boost | Example: Logistic Regression,"
      ],
      "metadata": {
        "id": "l5KGs9aeJ_8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential ML Models Details:\n",
        "\n",
        "Ref:\n",
        "1. https://www.youtube.com/watch?v=z18nw4adsx4"
      ],
      "metadata": {
        "id": "UtB8zuDkIEp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/TheDataFestAI/Learning_AI_ML/assets/108976294/b92328f2-abed-4a63-9b96-659eae9afc23)"
      ],
      "metadata": {
        "id": "5clCQFRv-FqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/easycloudapi/python_utility/assets/108976294/6d6339d9-3a5f-4465-8983-e3d7b4e58f19)"
      ],
      "metadata": {
        "id": "LG2aJ5JNAGiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID | Model Type | Model Name | Model Details | Pros - Cons\n",
        ":- | :--------- | :--------- | :------------ | -------\n",
        "1 | Supervised - Regression  | **Linear Regression** | 1. Data will be plotted into x,y axis grap <br> 2. Need to find best fit line which connect or minimize the distance from all data points <br> 3. this best fit line has the equation `y = mx + c` and act as a model | Pros: This is simple and easy to implement <br> **Cons:** In real world, its not possible to craete a best fit line which connect all data points.\n",
        "2 | Supervised - Regression | **Decision Tree** | 1. It will break into multiple rules like tree like structure, then it will predict | pros: It has no complex algorithm <br> **Cons:** Model can be overfitted and small changes in feature can impact the model\n",
        "3 | Supervised - Regression | **Random Forest** | 1. It will create multiple decission trees with random rows and columns <br> 2. In prediction, Random forest will output the average from all decission trees. <br> 3. Its an **Ensemble Learning with parralel way**. |\n",
        "4 | Supervised - Regression | **Ada Boost** <br> Adaptive Boosting | 1. In Boost, model assign initial weights to all observations <br> 2. All rows are qqually important at beginning, then it changes the weights based on previous mistakes the model made <br> 3. Its an **Ensemble Learning with sequential way**. <br> 4. Here all trees will be look like stumbs (only 1 level of decission trees) <br> 5. weights will be changed/adjusted based on the **Residuals (errors)**. More Residuals, more weights will be added | Pros: It will be better model <br> **Cons:** Its a black box model and also need more resources, data to train the model.\n",
        "5 | Supervised - Regression | **Gradient Boost** | 1. Its also an **Ensemble Learning with sequential way**. <br> 2. First, its create a base average model and then computeit residuals/ errors <br> 3. based on residuals, model try to minimize the residuals to fit the model and update the base model | Pros: Its very famous and most used model in kaggle competition <br> **Cons:** Its a complex model\n",
        "6 | Supervised - Regression | **XG Boost** <br> Exponential Gradient Boost | 1. Its different from Gradient Boost model |\n",
        "7 | Supervised - Classification | **Logistic Regression** | 1. Its only work with sigmoid equation (inverse of log function) <br> | **Cons:** It may not give accurate result all the time\n",
        "8 | Supervised - Classification | **KNN** <br> K-Nearest Neighbors | 1. It will try to allocate neighbors to all these individual observations and get the mod of neighbors to predict |\n",
        "9 | Supervised - Classification | **SVM** <br> Support Vector Machines | 1. Its try to create some decission boundaries <br> 2. In real world, its difficult to find simple decission boundaries <br> 3. So we need to specify **Hyperplanes** and **Kernels** to build complex decission boundaries | **Cons:**\n",
        "10 | Unsupervised | **K-means Clustering** | 1. It will create the group or cluster of similar data <br> 2. To find the similar data, its checking the distances between dataplots and if distance is less, create the group with that elements <br> 3. Check **Centroid** options for K-means |\n",
        "11 | Unsupervised | **Collaborative filtering** | 1. For an example, It will create matrix like table with User and Items. <br> 2. Then it will check for other similar users and predict the items | Pros: Used for recommendation systems\n",
        "12 | | | |\n",
        "\n"
      ],
      "metadata": {
        "id": "Zxags5y44b5t"
      }
    }
  ]
}